\chapter{Úvod do teórie strojového učenia}

Predstavme si, že chceme vedieť na základe nejakých vstupných dát $x$
predpovedať výstupné dáta $y$. Napríklad, môžme chcieť na základe rozlohy
bytu vedieť predpovedať jeho cenu. Alebo vedieť z obrázku (vo formáte
$32 \times 32$ čiernobielych pixelov) povedať, či v ňom nachádza mačka
alebo nie.

Snažíme sa teda zachytiť nejaký vzťah medzi dátami $x$ a $y$. To, akým
spôsobom to budeme robiť, je nasledovné: nejakým procesom $P$ získame $t$
\emph{trénovacích príkladov} $(x_1, y_1), \ldots, (x_t, y_t)$. Na základe
týchto príkladov budeme chcieť navrhnút nejakú funkciu $h$, ktorá bude
vedieť podľa vstupu $x$ predpovedať výstup $y$ s dostatočnou presnosťou.

Prirodzený spôsob, akým môžeme merať úspešnosť funkcie $h$, je podľa
toho, ako sa jej darí na našich $t$ príkladoch. Dobrá funkcia $h$ ale
bude schopná aj \emph{zovšeobecňovať}: bude sa jej dariť dobre nielen
na našich príkladoch, ale aj na ľubovoľných dátach, ktoré vieme získať
tým istým procesom $P$.

Ilustrujeme dôležitosť zovšeobecňovania: predstavme si, že funkciu $h$ 
zostrojíme tak, že si ``zapamätáme'' všetky trénovacie príklady. Ak je 
na vstupe $x$, ktoré bolo medzi trénovacími príkladmi, vrátime 
príslušné $y$. Takáto funkcia $h$ má zrejme nulovú chybu na trénovacích 
príkladoch (pokiaľ pre jedno $x$ je vždy len jedno správne $y$), ale 
mimo nich sa jej vôbec nebude dariť.




\section{Matematický model}

Proces, ktorým získavame dáta $(x, y)$, vieme formalizovať ako
pravdepodobnostné rozdelenie $P$. Každému možnému pozorovaniu teda
priradíme nejakú pravdepodobnosť, že si ho vytiahneme. Trénovacie
príklady získame ako $t$ nezávislých vzoriek z rozdelenia $P$.
Množinu všetkých možných vstupov $x$ označíme $X$, množinu možných
výstupov $y$ označíme $Y$.

Postup, ktorým zostrojíme funkciu $h$, vieme formalizovať ako algoritmus,
ktorý na vstupe dostane niekoľko trénovacích príkladov
$(x_1, y_1), \ldots, (x_t, y_t)$, a na výstupe vráti funkciu $h : X \to Y$,
ktorú budeme volať \emph{hypotéza}. Množinu všetkých možných funkcii,
ktoré môže náš algoritmus vrátiť, budeme volať \emph{množina hypotéz}
a budeme ju značiť $H$. V tejto kapitole sa nebudeme zaoberať výpočtovou stránkou
strojového učenia, takže od detailov ako časová zložitosť, ..., abstrahujeme.

\paragraph{Chyba jednej hypotézy.}
Ako vyjadriť mieru toho, že sa hypotéze ``dobre darí''? Spravíme tak
pomocou \emph{chybovej funkcie} $\err : Y \times Y \to \reals^+$,
ktorej význam je nasledovný: $\err(y, y')$ vyjadruje, ako veľmi
sa od seba líšia výstupy $y$ a $y'$.

Pomocou tejto funkcie vieme odmerať očakávanú chybu, ktorú spraví
hypotéza $h$, ak jej na vstupe dáme náhodne vybraný príklad $(x, y)$
z rozdelenia $P$. Túto chybu budeme tiež označovať $\err$, a vypočítame
ju nasledovne:
$$\err(h) = \E_{x, y} \left[ \err(h(x), y) \right].$$
Pod $\E_{x, y}$ sa rozumie stredná hodnota cez $(x, y)$
z pravdepodobnostného rozdelenia $P$, teda $(x, y) \sim P$.

Niekedy nás bude zaujímať ale aj chyba, ktorú hypotéza nadobúda na
trénovacích dátach $T$. Aby sme tieto dve chyby rozlíšili, prvú uvedenú
chybu budeme volať \emph{testovacia chyba} a druhú uvedenú
\emph{trénovacia chyba}. Trénovaciu chybu budeme označovať $\err_T(h)$
a vieme ju vypočítať nasledovne:
$$\err_T(h) = \E_{x_i, y_i} \left[ \err(h(x_i), y_i) \right] = \frac{1}{t} \cdot \sum_{i=1}^t \err(h(x_i), y_i).$$

Algoritmus sa snaží nájsť takú hypotézu $\hat{h}$, ktorej testovacia
chyba bude minimálna. Pretože ale nemá presný popis rozdelenia $P$ ale
iba niekoľko trénovacích príkladov, v istom zmysle najlepšie, čo môže
spraviť, je minimalizovať trénovaciu chybu hypotézy.

Uvedieme ešte príklady používaných chybových funkcii.
Pri klasifikácii sa zvykne používať chybová funkcia
$$
  \err(y, y') = \left\{
    \begin{array}{ll}
      0, & \text{ak}\ y = y' \\
      1, & \text{inak}
    \end{array}
  \right.
$$
a potom zrejme
$$\E_{x, y} \left[ \err(h(x), y) \right] = \prob_{x, y} \left( h(x) \neq y \right).$$
Pri regresii máme viacero možností, bežnými voľbami sú kvadratická chyba
$(y - y')^2$ a absolútna chyba $|y - y'|$.

\paragraph{Očakávaná testovacia a trénovacia chyba.}
Vyššie uvedeným spôsobom vieme porovnávať dve rôzne hypotézy. Nevieme
ale týmto spôsobom porovnať dve rôzne trénovacie algoritmy. Výstupná
hypotéza algoritmu, ktorú budeme označovať $\hat{h}$, totiž závisí
od toho, aké trénovacie príklady sme si s rozdelenia $P$ vytiahli,
a tiež od prípadnej náhody ak je algoritmus randomizovaný.

Pre jednu trénovaciu množinu $T_1$ môže byť lepší jeden algoritmus,
pre druhý množinu $T_2$ zas môže dávať lepší výsledok druhý algoritmus.
Vieme ale vypočítať strednú hodnotu testovacej chyby (\emph{očakávanú
testovaciu chybu}), rátanú cez vyššie uvedené náhodné faktory:
$$\E_T \left[ \err(\hat{h}) \right] = \E_T \left[ \E_{x,y} \left[ \err(\hat{h}(x), y) \right] \right].$$

Podobným spôsobom vieme vypočítať strednú hodnotu trénovacej chyby
(\emph{očakávanú trénovaciu chybu}):
$$\E_T \left[ \err_T(\hat{h}) \right] = \E_T \left[ \E_{x_i,y_i} \left[ \err(\hat{h}(x), y) \right] \right].$$

V nasledujúcom texte si dovolíme vynechávať premenné, cez ktoré prebiehajú
stredné hodnoty, všade tam, kde budú zrejmé z kontextu.




\section{Analýza veľkostí chýb}

V tejto časti sa podrobnejšie pozrieme na to, ako závisia vyššie
uvedené metriky (tj. očakávaná testovacia a očakávaná trénovacia chyba)
od veľkosti trénovacej množiny $t = |T|$ a od veľkosti množiny hypotéz $H$.
V celej časti budeme predpokladať, že úloha je regresného
charakteru a chyba sa meria ako kvadratická odchýlka, teda
$$\err(y, y') = (y - y')^2.$$



\subsection{Teoretické limity}
Najprv sa ale pozrieme na teoretické limity toho, ako dobrá vôbec
môže byť nejaká \emph{funkcia}. Označme $h^{\square}$ najlepšiu možnú
funkciu, nemusí byť nutne z $H$. Teda
$$h^{\square} = \argmin_h \left( \err(h) \right) = \argmin_h \left( \E_{x,y} \left[ (h(x) - y)^2 \right] \right).$$
Jediné obmedzenia kladené na $h$ sú, že je to funkcia: pre každé $x$
musí vrátiť vždy jednu a tú istú hodnotu. Distribúcia $P$ ale nemusí
pre dané $x$ vždy vrátiť to isté $y$: môže byť zašumená, alebo jednoducho
$x$ neobsahuje dostatočnú informáciu na to, aby sme vedeli predpovedať $y$.
Napríklad, ak podľa plochy bytu určujeme jeho cenu, niektoré dva byty 
môžu mať rovnakú plochu a predsa rôznu cenu. Ako uvidíme, tento 
nedeterminizmus je jediný dôvod, prečo hypotéza $h^{\square}$ nemusí 
mať nulovú chybu.

Chybu ľubovoľnej hypotézy $h$ vieme upraviť nasledovne:
\begin{align}
  \err(h)
    &= \E_{x,y} \left[ (h(x) - y)^2 \right] \\
    &= \E_{x} \left[ \E_{y|x} \left[ (h(x) - y)^2 \right] \right]
\end{align}
Pozrime sa na vnútornú strednú hodnotu. V nej je $x$ konštanta, a
teda aj $h(x) = c$ je konštanta. Aká konštanta minimalizuje danú
strednú hodnotu? Nie je ťažké vidieť (napríklad zderivovaním), že
minimum sa nadobúda pre $c = \E[y]$. Takže
$$h^{\square}(x) = \E_{y|x}[y],$$
a jeho testovacia chyba je
$$\err(h^{\square}) = \E_{x} \left[ \E_{y|x} \left[ (y - \E[y]) \right] \right] = \E_{x} \left[ \Var_{y|x}(y) \right].$$
Vidíme teda, že pokiaľ je $y$ jednoznačne určené $x$-om, tak
$h^{\square}$ bude mať nulovú chybu.



\subsection{Bias-variance tradeoff}
V tomto odseku si ukážeme zaujímavý výsledok, ktorý nám za určitých
predpokladov umožňuje vyjadriť chyby pomocou iných, jasnejších veličín:
tzv. \emph{výchylky} a \emph{rozptylu}.
Označme najlepšiu hypotézu z množiny $H$ ako $h^\star$, teda
$$h^\star = \argmin_h \left( \err(h) \right).$$

Budeme upravovať výraz reprezentujúci očakávanú testovaciu chybu.
\begin{align}
  \chalg
    &= \E_T \left[ \err(\hat{h}) \right]
    &= \E_T \left[ \E_{x,y} \left[ (\hat{h}(x) - y)^2 \right] \right] \\
    &= \E_T \left[ \E_{x,y} \left[ \left( (\hat{h}(x) - h^\star(x)) + (h^\star(x) - y) \right)^2 \right] \right]
\end{align}
V tomto momente prichádza netriviálny \hyperref[tradeoff:tech]{technický krok},
ktorý si vyžaduje dodatočné predpoklady. Tieto technické detaily prenecháme
na koniec časti, sústreďme sa na to hlavné.
$$
  \chalg
    = \E_T \left[ \E_{x,y} \left[ (\hat{h}(x) - h^\star(x))^2 \right] \right]
    + \E_T \left[ \E_{x,y} \left[ (h^\star(x) - y)^2 \right] \right]
$$
Druhý zo sčítancov sa dá ešte zjednodušiť. Kedže $h^\star$ ani $y$
nezávisia od trénovacích dát, môžeme sa zbaviť vonkajšej strednej
hodnoty. Dostávame tak výslednú rovnosť
\begin{equation}
  \chalg
    = \underbrace{\E_T \left[ \E_{x,y} \left[ (\hat{h}(x) - h^\star(x))^2 \right] \right]}_{\text{rozptyl}}
    + \underbrace{\E_{x,y} \left[ (h^\star(x) - y)^2 \right]}_{\text{výchylka}}.
\label{biasvarianceodvodenie}
\end{equation}

Prvý zo sčítancov budeme volať \emph{rozptyl}. Trénovací algoritmus
s malým rozptylom vracia funkcie, ktoré sú blízko optima v množine $H$.
Tým, že mu zväčšíme množinu trénovacích dát, si veľmi neprilepšíme.
Naopak, algoritmus s veľkým rozptylom vracia funkcie ďaleko od optima,
teoreticky by sme sa teda vedeli k optimu priblížiť tým, že zväčšíme
množstvo trénovacích dát.

\medskip

Druhý zo sčítancov budeme volať \emph{výchylka}. Vyjadruje chybu, ktorá
je spôsobená tým, že sa náš algoritmus obmedzil na nejakú konkrétnu
množinu hypotéz $H$. Čím väčšia množina hypotéz, tým menšia výchylka
(nakoľko $h^\star$ je najlepšia hypotéza v množine $H$, jej zväčšením
si môžeme iba prilepšiť). Zložitejšia množina hypotéz ale ľahšie
``napasuje'' na ľubovoľné trénovacie dáta. To zvyšuje riziko toho,
že výsledná hypotéza bude špecifická pre obdržané dáta, a nebude
schopná zovšeobecňovať mimo nich. Je teda potreba väčšieho množstva
trénovacích dát.

\medskip

Ak máme fixné trénovacie dáta $T$, pri voľbe množiny hypotéz $H$ sa snažíme
nájsť kompromis medzi malým rozptylom a malou výchylkou. Zložité $H$
bude mať malú výchylku ale veľký rozptyl, čo vedie k tzv. \emph{preučeniu}.
Jednoduché $H$ bude mať malý rozptyl, ale veľkú výchylku, tzv. \emph{podučenie}.

Na obrázku \ref{img:fitting} ilustrujeme oba koncepty. Úlohou je modelovať
kvadratickú funkciu. Ak za množinu hypotéz zvolíme lineárne funkcie,
ich chyby sa nebudú veľmi od seba líšiť, ale všetky budú zle. Ak za
množinu hypotéz zvolíme polynómy nejakého vysokého stupňa, ľahko
nájdeme polynóm prechádzajúci cez trénovacie dáta, avšak mimo nich
bude dávať výsledky úplne mimo.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{obrazky/fitting1.pdf}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\linewidth]{obrazky/fitting9.pdf}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\linewidth]{obrazky/fitting2.pdf}
  \end{subfigure}
  \caption{Podučenie, preučenie, akurát.}
  \label{img:fitting}
\end{figure}

\medskip

Výchylku vieme upraviť ďalej. Hypotéza $h^\star$ ani $y$ nezávisia
od trénovacej množiny $T$. Z ich pohľadu sú teda testovacie dáta $x, y$
a trénovacie dáta $x_i, y_i$ nerozoznateľné. Takže na meranie chyby
$h^\star$ môžeme použiť trénovacie dáta, berúc v úvahu ich náhodný
výber:
\begin{align}
  \vychylka
    &= \E_T \left[ \E_{x_i, y_i} \left[ (h^\star(x_i) - y_i)^2 \right] \right] \\
    &= \E_T \left[ \E_{x_i, y_i} \left[ \left( (h^\star(x_i) - \hat{h}(x_i)) + (\hat{h}(x_i) - y_i) \right)^2 \right] \right]
\end{align}
Použitím ďalšieho technického kroku dostaneme:
\begin{align}
  \vychylka &= \underbrace{\E_T \left[ \E_{x_i, y_i} \left[ (h^\star(x_i) - \hat{h}(x_i))^2 \right] \right]}_\rozptylt
    + \underbrace{\E_T \left[ \E_{x_i, y_i} \left[ (\hat{h}(x_i) - y_i)^2 \right] \right]}_\chalgt
\end{align}

Prvý zo sčítancov budeme volať \emph{trénovací rozptyl}.
Uvedomme si, že pre ľubovoľné trénovacie dáta $T$ platí
$$\err_T(\hat{h}) \leq \err_T(h^\star),$$
nakoľko $\hat{h}$ je optimálna hypotéza pre danú množinu trénovacích
dát. Hypotéza $h$ síce je najlepšia pre $H$, trénovacie dáta sú ale len
malá vzorka z $H$.
Trénovací rozptyl teda môžeme chápať ako mieru toho, ako veľmi
reprezentatívnu vzorku trénovacích dát sme dostali. Čím menší je,
tým viac reprezentatívna je naša vzorka.

\medskip

Druhý zo sčítancov je očakávaná trénovacia chyba. Je to stredná hodnota
chyby, ktorej sa dopustí výstup z algoritmu $\hat{h}$ na tých istých
dátach, pomocou ktorých sme $\hat{h}$ zostrojili.

\medskip

Platí
$$ \chalgt \leq \vychylka \leq \chalg. $$
Na konkrétnych trénovacích dátach ale nemusí platiť, že trénovacia
chyba je menšia ako testovacia chyba: mohli sme si (síce s malou
pravdepodobnosťou, ale predsa) vytiahnuť zlé trénovacie dáta, na
ktorých sa žiadnej hypotéze z $H$ nedarí.

\medskip

Na základe dosiaľ uvedeného vieme graficky znázorniť, ako sa zhruba správajú
očakávaná testovacia chyba, rozptyl, výchylka, trénovací rozptyl a očakávaná
trénovacia chyba, v závislosti od veľkosti trénovacej množiny (obrázok 
\ref{img:train}) a od zložitosti množiny hypotéz (obrázok 
\ref{img:hypo}).

\begin{figure}
  \centering
  \includegraphics[scale=0.8]{obrazky/krivky1.pdf}
  \caption{Závislosť chyby algoritmu od počtu trénovacích príkladov $t$.}
  \label{img:train}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.8]{obrazky/krivky2.pdf}
  \caption{Závislosť chyby algoritmu od zložitosti množiny hypotéz $H$.}
  \label{img:hypo}
\end{figure}

\input cast1-technicke-detaily

\iffalse
\paragraph{Technické detaily.} \label{tradeoff:tech}
Nakoniec sa vyjadríme k spomínanému technickému kroku. Začneme jeho
znením a potom uvedieme jeho predpoklady.

\begin{theorem} \label{theorem:proj1}
  Predpokladajme, že vstupom do hypotéz sú vektory reálnych čísel
  (tj. $X = \reals^n$), cieľom je predpovedať jedno reálne číslo
  (tj. $Y = \reals$), a že pravdepodobnostné rozdelenie $P$ je spojité.
  Nech množina hypotéz $H$ je uzavretá na lineárne kombinácie a na
  limity (teda ak postupnosť funkcii v $H$ konverguje, jej limita
  je tiež v $H$).
  Ďalej predpokladajme, že trénovací algoritmus vždy vráti takú funkciu
  $\hat{h} \in H$, ktorá minimalizuje trénovaciu chybu. Inak zapísané,
  $$\hat{h} = \argmin_{h \in H} \left( \E_T \left[ \err_T(h) \right] \right).$$
  
  Potom platí
  $$\E_T \left[ \E_{x,y} \left[ \left( (\hat{h}(x) - h^\star(x)) + (h^\star(x) - y) \right)^2 \right] \right] = \E_T \left[ \E_{x,y} \left[ (\hat{h}(x) - h^\star(x))^2 \right] \right] + \E_T \left[ \E_{x,y} \left[ (h^\star(x) - y)^2 \right] \right]$$
\end{theorem}
\begin{remark}
  Dokazovaná rovnosť je ekvivalentná s nasledovnou, stručnejšou:
  $$\E_T \left[ \E_{x, y} \left[ (\hat{h}(x) - h^\star(x)) \cdot (h^\star(x) - y) \right] \right] = 0.$$
  Túto kratšiu verziu získame roznásobením a použitím linearity strednej
  hodnoty. V dôkaze budeme dokazovať túto rovnosť.
\end{remark}
\begin{remark}
  Všimnite si, že potrebujeme uzavretosť množiny $H$ na limity na to,
  aby vôbec $\argmin_{h \in H} (\ldots)$ existovalo. Vo všeobecnosti
  nemusí existovať taká funkcia, ale môže existovať nekonečná postupnosť
  funkcii, každá ďalšia lepšia, ako tá predchádzajúca. (Inak povedané,
  vo všeobecnosti neexistuje minimum, iba infimum.)
\end{remark}
\begin{remark}
  Je namieste otázka, či je $\argmin_{h \in H} (\ldots)$ dobre definované,
  teda či je taká funkcia $h$ práve jedna. Za chvíľu uvidíme, že naše
  predpoklady to zaručujú.
\end{remark}
\begin{remark}
  Veta by sa dala rozšíriť aj na iné množiny $X, Y$, napríklad keď
  predpovedaná premenná je vektor ($Y = \reals^m$), ... Možno ani $P$
  nemusí byť spojité. Pre jednoduchosť argumentu ale budeme uvažovať
  vetu tak, ako je popísaná vyššie.
\end{remark}
\begin{remark}
  Predpoklady vety sú značne obmedzujúce. Napríklad si uvedomte, že
  ju nie je možné použiť na klasifikáciu, či dokonca ani na ľubovoľnú
  ohraničenú regresiu (kde sú výstupné hodnoty $y$ ohraničené). Ale
  taká je teória.
\end{remark}

Pri našom dôkaze využijeme niekoľko vlastností funkcii, ktoré uvádzame
v nasledujúcom odseku. Skúsený čitateľ-matematik ho môže preletieť.

\begin{definition}
  (Skalárny súčin.) Nech $f, g$ sú funkcie z $X$ do $\reals$,
  z nejakej príjemne sa správajúcej množiny funkcii (tj. rovnomerne
  spojité, ..., čokoľvek, aby nasledujúce argumenty prešli). Definujeme
  ich skalárny súčin $\langle \cdot, \cdot \rangle$ ako
  \begin{align}
    \langle f, g \rangle
      &= \int f(x) \cdot g(x)\ d\rho x \\
      &= \E_x \left[ f(x) \cdot g(x) \right],
  \end{align}
  kde $\rho$ je hustota pravdepodobnosti rozdelenia $P$.
  Rozmyslite si, že takto definovaný skalárny súčin má všetky vlastnosti,
  ktoré sa bežne požadujú od skalárnych súčinov:
  \begin{itemize}
    \item Je symetrický od svojich argumentov, teda $\langle f, g \rangle = \langle g, f \rangle$.
    \item Je lineárny: $\langle f, g + h \rangle = \langle f, g \rangle + \langle f, h \rangle$
      a tiež $\langle k \cdot f, g \rangle = k \cdot \langle f, g \rangle$.
    \item $\langle f, f \rangle \geq 0$ pre ľubovoľné $f$, pričom rovnosť
      nastáva práve vtedy, keď je $f$ konštantne nulové.
  \end{itemize}
\end{definition}

\begin{definition}
  (Kolmosť.) Dve funkcie $f, g$ sú na seba kolmé, ak ich skalárny
  súčin je $0$. Značíme $f \perp g$.
\end{definition}

\begin{definition}
  (Norma.) Podľa skalárneho súčinu definujeme normu funkcie (jej ``dĺžku''):
  $$\norm{f} = \sqrt{\langle f, f \rangle} = \sqrt{\E_x \left[ f^2(x) \right] }$$
  Takto definovaná norma spĺňa \emph{trojuholníkovú nerovnosť}: pre ľubovoľné
  funkcie $f, g$ platí
  $$\norm{f} + \norm{g} \geq \norm{f + g}.$$
  Podľa normy príslušne definujeme vzdialenosť dvoch funkcii:
  $$d(f, g) = \norm{f - g}.$$
  Táto vzdialenosť nám definuje (euklidovskú) metriku nad funkciami, podľa
  ktorej vieme definovať limity a konvergenciu.
\end{definition}

\begin{lemma}
  (Pytagorova veta.) Nech $f \perp g$. Potom platí:
  $$\norm{f}^2 + \norm{g}^2 = \norm{f + g}^2$$
\end{lemma}
\begin{proof}
  Pozrime sa na pravú stranu. Iba v nej zapíšeme normu ako skalárny
  súčin a využijeme jeho linearitu a symetriu:
  \begin{align}
    \norm{f + g}^2
      &= \langle f + g, f + g \rangle \\
      &= \langle f, f \rangle + \langle g, g \rangle + 2 \cdot \langle f, g \rangle \\
      &= \norm{f}^2 + \norm{g}^2 + 2 \cdot \langle f, g \rangle
  \end{align}
  Vďaka $f \perp g$ je posledný sčítanec nulový, čím dostávame
  dokazované tvrdenie.
\end{proof}

\begin{definition}
  (Projekcia na množinu.) \emph{Projekcia} funkcie $f$ na množinu $H$
  je taká funkcia $f_H$, ktorá je spomedzi funkcii v $H$ k našej
  funkcii $f$ najbližšie:
  $$f_H = \argmin_{h \in H} d(f, h)$$
\end{definition}
\begin{remark}
  Ako sa už spomínalo, nie je zrejmé, či je projekcia dobre definovaná.
  Preto v nasledujúcej lemme definujeme $f_H$ trochu iným spôsobom, ako
  jednu z možno viacerých funkcii, ktoré minimalizujú vzdialenosť k $H$.
\end{remark}

\begin{lemma} \label{lem:projperp}
  (Kolmosť projekcie.) Pre ľubovoľnú funkciu $h \in H$ platí $h \perp f - f_H$.
  Inými slovami, kolmica z $f$ na $H$ je kolmá na celé $H$.
\end{lemma}
\begin{proof}
  Sporom, predpokladajme, že $h \not\perp f - f_H$. Takže
  $\langle h, f - f_H \rangle \neq 0$. Ukážeme, že potom existuje
  v $H$ funkcia, ktorá je k funkcii $f$ bližšie, ako funkcia $f_H$.
  To bude spor s definíciou $f_H$.
  
  Pozrime sa na všetky funkcie, ktoré ležia na priamke
  $f_H + \Delta \cdot h$, pre $\Delta \in \reals$. Celá táto priamka leží
  v množine $H$, nakoľko množina $H$ je uzavretá na lineárne kombinácie
  a obe funkcie $f_H, h \in H$. Každú bod priamky vieme asociovať
  s jedným reálnym číslom $\Delta$. Pozrime sa na ich vzdialenosti od
  funkcie $f$ v závislosti od $\Delta$:
  \begin{align}
    \text{dist}(\Delta)
      &= d(f, f_H + \Delta \cdot h) \\
      &= \langle (f - f_H) + \Delta \cdot h, (f - f_H) + \Delta \cdot h \rangle \\
      &= \langle f - f_H, f - f_H \rangle + 2\Delta \cdot \langle h, f - f_H \rangle + \Delta^2 \cdot \langle h, h \rangle
  \end{align}
  Pozrime sa na deriváciu tejto funkcie. Podľa definície $f_H$ by
  malo byť $f - f_H$ najkratšie možné, teda pre $\Delta = 0$ by mala
  funkcia $\text{dist}$ nadobúdať minimum, a teda mať tam nulovú
  deriváciu. Uvidíme, že tomu tak nie je:
  \begin{align}
    \frac{\partial \text{dist}}{\partial \Delta} \left( 0 \right)
      &= \lim_{\Delta \to 0} \left( \frac{\text{dist}(\Delta) - \text{dist}(0)}{\Delta} \right) \\
      &= \lim_{\Delta \to 0} \left( \frac{2\Delta \cdot \langle h, f - f_H \rangle + \Delta^2 \cdot \langle h, h \rangle}{\Delta} \right) \\
      &= 2 \cdot \langle h, f - f_H \rangle
  \end{align}
  To je nenulové, nakoľko $h \not \perp f - f_H$. Čo je hľadaný spor.
\end{proof}

\begin{lemma}
  Projekcia na množinu $H$ je dobre definovaná, teda vždy existuje
  nanajvýš jedna funkcia $f_H \in H$, ktorá minimalizuje vzdialenosť
  k $f$.
\end{lemma}
\begin{proof}
  Predpokladajme, že také funkcie sú dve, označme ich $g, h$. Ukážeme,
  že potom nutne $g = h$. Podľa predchádzajúcej lemmy sú obe kolmice
  $f - g$ aj $f - h$ kolmé na celé $H$. Špeciálne sú teda obe kolmé
  na $g$ aj na $h$, odkiaľ dostaneme:
  \begin{align}
    \langle f - g, g \rangle = 0 \\
    \langle f - h, g \rangle = 0 \\
    \langle f - g, h \rangle = 0 \\
    \langle f - h, h \rangle = 0
  \end{align}
  Tieto rovnosti upravíme pomocou linearity skalárneho súčinu. Dostaneme
  z nich
  $$\langle g, g \rangle = \langle g, h \rangle = \langle h, g \rangle = \langle h, h \rangle.$$
  Nakoniec, pozrime sa na normu funkcie $g - h$. Pomocou predchádzajúceho
  výsledku ukážeme, že musí byť nulová, čo môže nastať jedine pre $g = h$:
  \begin{align}
    \norm{g - h}
      &= \sqrt{\langle g - h, g - h \rangle} \\
      &= \sqrt{\langle g, g \rangle - \langle g, h \rangle - \langle h, g \rangle + \langle h, h \rangle} \\
      &= 0
  \end{align}
\end{proof}

\begin{lemma} \label{lem:hsq}
  Hypotéza $h^\star$ je projekciou $h^\square$ na $H$, inak zapísané
  $h^\star = h^\square_H$. Tvrdíme teda, že najlepšia funkcia z množiny
  $H$ je tá, ktorá sa (v euklidovskej metrike) najmenej líši od $h^\square$.
\end{lemma}
\begin{proof}
  Vychádzajme z definície $h^\star$.
  \begin{align}
    h^\star
      &= \argmin_{h \in H} \E_{x,y} \left[ (h(x) - y)^2 \right] \\
      &= \argmin_{h \in H} \E_{x,y} \left[ \left( (h(x) - h^\square(x)) + (h^\square(x) - y) \right)^2 \right] \\
      &= \argmin_{h \in H} \left(
        \begin{array}{ll}
          &\E_{x,y} \left[ (h(x) - h^\square(x))^2 \right] \\
          + &\E_{x,y} \left[ (h^\square(x) - y)^2 \right] \\
          + 2 \cdot &\E_{x,y} \left[ (h(x) - h^\square(x)) \cdot (h^\square(x) - y) \right]
        \end{array}
        \right)
  \end{align}
  Druhý sčítanec je konštanta, teda nás pri minimalizácii nemusí zaujímať.
  Môžeme ho teda ignorovať. Tretí sčítanec vieme upraviť nasledovne:
  \begin{align}
    \E_{x,y} \left[ (h(x) - h^\square(x)) \cdot (h^\square(x) - y) \right]
      &= \E_x \left[ \E_{y|x} \left[ (h(x) - h^\square(x)) \cdot (h^\square(x) - y) \right] \right] \\
      &= \E_x \left[ (h(x) - h^\square(x)) \cdot \E_{y|x} \left[ h^\square(x) - y \right] \right] \\
      &= 0
  \end{align}
  A teda je to tiež konštanta. Dostávame tak
  $$h^\star = \argmin_{h \in H} \E_{x,y} \left[ (h(x) - h^\square(x))^2 \right],$$
  čo je presne definícia projekcie $h^\square$ na množinu $H$.
\end{proof}

Vyzbrojení týmito znalosťami, môžeme sa vrhnúť na dôkaz vety \ref{theorem:proj1}.
Pripomeňme si na začiatok dokazovanú rovnosť:
$$\E_T \left[ \E_{x, y} \left[ (\hat{h}(x) - h^\star(x)) \cdot (h^\star(x) - y) \right] \right] = 0.$$

\begin{proof}
  Ľavú stranu dokazovanej rovnosti vieme prepísať do nasledovného,
  ekvivalentného tvaru:
  $$ \E_T \left[ \E_{x, y} \left[ (\hat{h}(x) - h^\star(x)) \cdot ((h^\star(x) - h^\square(x)) + (h^\square(x) - y)) \right] \right] $$
  Vieme, že $\varepsilon := h^\square(x) - y$ sa správa pre dané $x$
  ako náhodná premenná, ktorá má strednú hodnotu $0$ a je nezávislá
  od ostatných premenných vystupujúcich vo výraze. Z výrazu ju teda
  môžeme vyhodiť, dostaneme tak
  $$ \E_T \left[ \E_{x, y} \left[ (\hat{h}(x) - h^\star(x)) \cdot (h^\star(x) - h^\square(x)) \right] \right] $$
  Stačí nám teda dokázať $\hat{h} - h^\star \perp h^\star - h^\square$.
  To ale vyplýva z nasledovnej úvahy. Pretože $h^\star = h^\square_H$,
  funkcia $h^\star - h^\square$ je kolmicou z funkcie $h^\square$ na
  množinu $H$. Je teda kolmá na celú množinu $H$. Stačí preto ukázať,
  že funkcia $\hat{h} - h^\star$ je z $H$; to ale plynie z uzavretosti
  $H$ na lineárne kombinácie.
\end{proof}

Podobným spôsobom sa dá dokázať aj korektnosť druhého technického
kroku. To prenechávame čitateľovi ako cvičenie.
\fi


\subsection{Bias-variance tradeoff, verzia 2.}
V literatúre pod názvom \emph{bias-variance tradeoff} vystupuje aj
podobný, ale predsa odlišný výsledok, ako bolo uvedené vyššie.
Ukážeme a odvodíme si ho.

\begin{theorem}
  Nech $y : X \to \reals$ je funkcia, ktorú sa snažíme modelovať.
  Predpokladajme, že sa dá rozložiť na časti: $y = f(x) + \varepsilon$,
  kde $\varepsilon$ hrá rolu šumu: je nezávislý od všetkého a
  $\E[\varepsilon] = 0$. Označíme jeho pravdepodobnostnú distribúciu
  $E$.
  
  Nech výstupom trénovacieho algoritmu je $\hat{f}$. Za chybovú
  funkciu zvoľme kvadratickú chybu. Chybu algoritmu vieme teda
  vypočítať nasledovne:
  $$\chalg = \E_{(x, y) \sim P, T \sim P^t, \varepsilon \sim E} \left[ (\hat{f}(x) - y)^2 \right].$$
  
  Tvrdíme, že sa dá rozložiť na tri nasledovné časti:
  $$
  \chalg
      = \underbrace{\Var(\hat{f}(x) - f(x))}_{\text{\normalfont rozptyl}}
      + \underbrace{(\E[\hat{f}(x)] - \E[f(x)])^2}_{\text{\normalfont výchylka}^2}
      + \underbrace{\Var(\varepsilon)}_{\text{\normalfont šum}}
  $$
\end{theorem}
\begin{remark}
  V poslednej rovnici sme kvôli stručnosti vynechali pri stredných
  hodnotách a rozptyloch premenné a distribúcie, z ktorých ich berieme.
  V dôkaze budeme vždy brať všetky premenné z ich príslušných distribúcii.
\end{remark}
\begin{remark}
  Funkcia $f$ hrá v podstate tú istú rolu, čo najlepšia možná hypotéza
  spomedzi všetkých funkcii (nielen tých v množine hypotéz), teda $h^\square$.
\end{remark}
\begin{remark}
  V tomto znení bias-variance tradeoff-u názvy \emph{rozptyl} a
  \emph{výchylka} zodpovedajú príslušným štatistickým/pravdepodobnostným
  pojmom.
\end{remark}
\begin{remark}
  Na rozdiel od predchádzajúcej verzie bias-variance tradeoff-u, tu
  nebudeme potrebovať žiadne dodatočné predpoklady od algoritmu ani
  od jeho množiny hypotéz. (Nemusí teda vracať hypotézu, ktorá je
  spomedzi hypotéz v $H$ najlepšia na daných trénovacích dátach.
  Takisto od množiny hypotéz nepožadujeme žiadne vlastnosti.)
\end{remark}
\begin{proof}
  Upravujme pôvodný výraz.
  \begin{align}
    \chalg
      &= \E \left[ (\hat{f}(x) - y)^2 \right] \\
      &= \E \left[ (\hat{f}(x) - f(x) - \varepsilon)^2 \right] \\
      &= \E \left[ (\hat{f}(x) - f(x))^2 \right] + \E \left[ \varepsilon^2 \right] - 2 \cdot \E \left[ \varepsilon \cdot (\hat{f}(x) - f(x)) \right] \\
      &= \E \left[ (\hat{f}(x) - f(x))^2 \right] + \E \left[ \varepsilon^2 \right]
  \end{align}
  Výraz sme upravili, roznásobili a využili linearitu strednej hodnoty.
  V poslednom kroku sme použili $\E[ab] = \E[a] \cdot \E[b]$, ktorý
  platí pre ľubovoľné nezávislé premenné, s $a := \varepsilon$,
  $b := \hat{f}(x) - f(x)$. Zamerajme sa ďalej na prvý sčítanec.
  \begin{align}
    \text{prvý sčítanec}
      &= \E \left[ (\hat{f}(x) - f(x))^2 \right] \\
      &= \E[\hat{f}(x)^2] + \E[f(x)^2] - 2 \cdot \E[\hat{f}(x) \cdot f(x)] \\
      &= (\Var(\hat{f}(x)) + \E[\hat{f}(x)]^2) + (\Var(f(x)) + \E[f(x)]^2) - 2 \cdot \E[\hat{f}(x) \cdot f(x)]
  \end{align}
  V poslednom kroku sme využili vzťah $\Var(a) = \E[a^2] - \E[a]^2$.
  Pokračujme ďalej v úpravách.
  \begin{align}
    \begin{split}
      \text{prvý sčítanec}
        &= \Var(\hat{f}(x)) + \Var(f(x)) + (\E[\hat{f}(x)] - \E[f(x)])^2 \\
        &\quad + 2 \cdot \E[\hat{f}(x)] \cdot \E[f(x)] - 2 \cdot \E[\hat{f}(x) \cdot f(x)]
    \end{split} \\
    &= \Var(\hat{f}(x)) + \Var(f(x)) + (\E[\hat{f}(x)] - \E[f(x)])^2 - 2 \cdot \Cov(\hat{f}(x), f(x)) \\
    &= \Var(\hat{f}(x) - f(x)) + (\E[\hat{f}(x)] - \E[f(x)])^2
  \end{align}
  Využili sme najprv vzťah $\Cov(a, b) = \E[ab] - \E[a] \cdot \E[b]$,
  a potom $\Var(a - b) = \Var(a) + \Var(b) - 2 \cdot \Cov(a, b)$.
  Keď to teda celé dáme do jednej rovnice, dostaneme
  $$
  \chalg
      = \underbrace{\Var(\hat{f}(x) - f(x))}_{\text{\normalfont rozptyl}}
      + \underbrace{(\E[\hat{f}(x)] - \E[f(x)])^2}_{\text{\normalfont výchylka}^2}
      + \underbrace{\Var(\varepsilon)}_{\text{\normalfont šum}}
  $$
\end{proof}




\section{Ako sa vysporiadať s preučením/podučením?}

V tejto časti sa budeme zaoberať otázkou: ``Ako zvoliť vhodne zložitú
množinu hypotéz?'' Ako sme videli, príliš jednoduché hypotézy vedú
k síce malému rozptylu, ale veľkej výchylke, zatiaľ čo príliš zložité
hypotézy vedú k malej výchylke, ale veľkému rozptylu.

Predstavme si, že máme na výber z viacerých množín hypotéz, čím ďalej
tým zložitejších:
$$H_1 \subseteq H_2 \subseteq H_3 \subseteq \ldots$$
Z ktorej množiny hypotéz chceme vybrať?

Pri trénovaní sa snažíme nájsť hypotézu $h$, ktorá minimalizuje chybu
na trénovacích dátach $\err_T(h)$. Táto chyba nám ale nehovorí nič o rozptyle.
Ak by sme si graficky znázornili testovacie a trénovacie chyby najlepších
hypotéz z jednotlivých množín, vyzeralo by to zhruba ako na obrázku
\ref{img:multimodels}.

\begin{figure}
  \centering
  \includegraphics[scale=1]{obrazky/multimodels.pdf}
  \caption{Testovacie (čierne) a trénovacie (biele) chyby v čím ďalej,
    tým zložitejších množinách hypotéz $H$.}
  \label{img:multimodels}
\end{figure}



\subsection{Regularizácia}

V tomto prístupe do minimalizovaného výrazu umelo pridáme člen, ktorý
aproximuje rozptyl: $\penal(h)$, pričom z čím zložitejšej množiny
hypotéza $h$ je, tým väčšia pokuta. Výstupom algoritmu potom je
$$\hat{h} = \argmin_{h \in H_1 \cup H_2 \cup \ldots} \left( \err_T(h) + \penal(h) \right).$$

Uvedomte si, že vrámci jednej množiny $H_i$ ostáva ako najlepšia
hypotéza stále tá istá, ako pred zavedením pokuty. V jednej množine
sú totiž všetky hypotézy penalizované rovnako, nerobí to teda rozdiel.
Penalizácia nám ale umožňuje ``férovejšie'' porovnávať hypotézy z rôznych
množín, nakoľko bez pokuty by na tom boli (neprávom) lepšie
zložitejšie hypotézy.

Množiny $H_i$ nemusia byť explicitné, môžu byť implicitne skryté v tom,
aký tvar má výraz $\penal(h)$. Do jednej množiny patria tie hypotézy,
ktoré majú rovnakú penalizáciu.

Vo všetkých prípadoch je pokuta parametrizovaná reálnym
parametrom $\lambda$ hovoriacim, ako veľké pokuty chceme udeľovať.
Veľké $\lambda$ (v porovnaní s trénovacou chybou) nám hovorí, že sa
snažíme hlavne dosiahnuť jednoduché hypotézy; s malým $\lambda$ zas
kladieme dôraz na hypotézy s menšou trénovacou chybou.

Uvedieme si niekoľko príkladov výrazov, ktoré môžu byť použité ako
pokuta. Budeme predpokladať, že celá množina hypotéz, z ktorej vyberáme
(tj. $H_1 \cup H_2 \cup \ldots$) je množina lineárnych funkcii
$\reals^n \to \reals$. Hypotézy majú teda tvar
$$h(x) = a_1x_1 + a_2x_2 + \ldots + a_nx_n.$$
\begin{itemize}
  \item $L_2$ regularizácia (známa aj ako \emph{ridge regression}).
    $$ \penal(h) = \lambda \cdot \norm{(a_1, a_2, \ldots, a_n)}^2 = \lambda \cdot (a_1^2 + a_2^2 + \ldots + a_n^2) $$
    Táto pokuta ``tlačí'' váhy nepotrebných atribútov do nuly, pričom
    väčšie váhy tlačí viac. Takže čím dôležitejší atribút, tým väčšiu
    váhu si môže dovoliť mať.
  \item $L_1$ regularizácia (známa aj ako \emph{lasso}).
    $$ \penal(h) = \lambda \cdot (|a_1| + |a_2| + \ldots + |a_n|) $$
    Opäť ``tlačíme'' nepotrebné atribúty do nuly, pričom ale všetky
    váhy tlačíme rovnako. To nám vie vynulovať nepotrebné atribúty,
    čo nám vie znížiť výpočtové nároky. Na druhej strane sa táto pokuta
    neoptimalizuje ľahko (z optimalizačného hľadiska).
\end{itemize}



\subsection{Holdout testing}

V tomto prístupe si rozdelíme dostupné dáta na dve časti:
trénovaciu množinu a \emph{validačnú množinu}. Pomocou validačnej
množiny budeme odhadovať testovacie chyby pre jednotlivé množiny
hypotéz, na základe ktorých zistíme, ktorá množina hypotéz je
pre náš problém najvhodnejšia. Konkrétnejšie:
\begin{enumerate}
  \item Trénovaciu množinu použijeme na natrénovanie hypotéz
    z jednotlivých množín.
  \item \label{holdout:step2} Ako odhad testovacej chyby jednotlivých
    hypotéz použijeme ich chybu na validačnej množine. Podľa týchto
    odhadov zistíme, ktorá množina hypotéz je pre náš problém najvhodnejšia.
  \item Použijeme všetky dáta, ktoré máme k dispozícii (tj. z trénovacej
    aj validačnej množiny), na natrénovanie najlepšej možnej hypotézy.
    Berieme samozrejme v úvahu iba hypotézy z tej množiny hypotéz, ktorú
    sme identifikovali ako najvhodnejšiu. Výsledná hypotéza je výstupom
    algoritmu.
\end{enumerate}
 
V kroku \ref{holdout:step2} je dôležité, aby bola validačná
množina nezávislá od trénovacej. Prečo je to dôležité? Môžeme uvažovať
extrémny prípad, keď je validačná množina totožná s trénovacou. Potom
ale ako náš ``odhad'' dostaneme trénovaciu chybu, ktorá rozhodne nie
je dobrým odhadom testovacej chyby. Nezávislosť nám teda zaručuje, že
odhad získaný na validačnej množine je dobrý.

\begin{comment}
  \begin{remark}
    Treba podotknúť, že chyba na validačnej množine je iba odhad. Ak
    by sme mali dostatočne veľa rôznych modelov, z ktorých vyberáme
    (tj. $H_1, H_2, \ldots$), pre niektorý z nich by sa mohlo stať čistou
    náhodou, že jeho validačná chyba je nízka, napriek tomu, že jeho
    skutočná testovacia chyba je vysoká.
    
    Toto je podobné, ako pri overovaní vedeckých hypotéz: napríklad si
    predstavme $10$ hypotéz, každá s $10\%$ šancou, že bude konzistentná
    s nazbieranými dátami. Potom môžeme očakávať, že jedna z nich bude
    konzistentná s nazbieranými dátami, napriek tomu, že všetky sú úplne
    náhodné.
    
    V oboch prípadoch sa to dá samozrejme eliminovať jedným spôsobom:
    viac dát.
  \end{remark}
\end{comment}

\paragraph{$k$-fold evaluation.} Pri holdout testovaní je dôležité mať
dobrý odhad testovacej chyby pre jednotlivé množiny hypotéz. Dát ale
môže byť málo, a v takom prípade môže byť odhad nestabilný/nepresný.
Môžeme ale experiment zopakovať niekoľkokrát: v každej iterácii teda
zvolíme inú trénovaciu a inú validačnú množinu, a dostaneme iný odhad
testovacej chyby. Keď tieto odhady spriemerujeme, dostaneme oveľa
presnejší odhad, ako keby sme vykonali iba jednu iteráciu.

V tomto konkrétnom prístupe je $k$ iterácii, a množiny sa volia
nasledovne: všetky dáta sa rozdelia na $k$ zhruba rovnako veľkých
a navzájom nezávislých množín $K_1, K_2, \ldots, K_k$. Následne,
v iterácii $i$ sa ako validačné dáta použije množina $K_i$. Všetko
ostatné budú trénovacie dáta.

\paragraph{Testovacia množina.} Ak chceme zmerať testovaciu chybu
výstupnej hypotézy, musíme si na to rezervovať ďalšiu časť dát:
\emph{testovaciu množinu}. Tú nepoužívame ani pri trénovaní, ani
pri validácii. Iba úplne na konci celého procesu na nej vypočítame
chybu našej hypotézy.

\begin{remark}
  ``Ak sa mi model trénuje príliš dobre, väčšinou to je veľmi zle!''
\end{remark}



\section{Cvičenia}

V nasledujúcich dvoch cvičeniach môžete predpokladať, že trénovací
algoritmus vždy vráti nejakú funkciu (nemusí byť len jedna)
s minimálnou chybou na trénovacích dátach.

\begin{exercise}
  Je rozumné predpokladať (a všade vyššie sme tak činili), že s väčším
  množstvom trénovacích dát sa nám bude testovacia chyba zmenšovať. Sú
  ale zostrojiteľné situácie, kedy tomu tak nie je. Nájdite jednu takú
  situáciu.
  
  Konkrétne, nájdite takú množinu hypotéz $H$ funkcii $\reals^n \to \reals$
  a pravdepodobnostné rozdelenie $P$, pre ktoré sa nám bude testovacia
  chyba so zvyšujúcim sa počtom trénovacích chýb \emph{zvyšovať}. Jediná
  podmienka je kladená na množinu hypotéz: pre každú možnú trénovaciu
  množinu $T$ musí existovať hypotéza v $H$, ktorá minimalizuje
  trénovaciu chybu. (Teda vždy musí existovať minimum, vo všeobecnosti
  existuje iba infimum.)
\end{exercise}

\begin{exercise}
  Za určitých podmienok ale skutočne platí, že viac trénovacích dát
  nám vo veľkom merítku neuškodí. Nech množina hypotéz $H$ je konečná
  a všetky jej funkcie ($\reals^n \to \reals$) sú ohraničené. Dokážte,
  keď $t \to \infty$, tak chyba hypotézy $\hat{h}$ sa bude blížiť k
  chybe najlepšej možnej hypotézy $h^\star$. Konkrétnejšie, dokážte
  $$\lim_{t \to \infty} \E_T \left[ \err(\hat{h}) - \err(h^\star) \right] = 0.$$
\end{exercise}

\begin{exercise}
  Dokážte korektnosť druhého technického kroku, v odvodení rozkladu
  výchylky na trénovací rozptyl a očakávanú trénovaciu chybu. Konkrétnejšie,
  dokážte
  $$ \E_T \left[ \E_{x_i, y_i} \left[ (h^\star(x_i) - \hat{h}(x_i)) \cdot (\hat{h}(x_i) - y_i) \right] \right] = 0. $$
  Predpoklady kladené na množinu hypotéz sú rovnaké: musí byť uzavretá
  na lineárne kombinácie a na limity.
\end{exercise}

\begin{exercise}
  Jednou výhodou $L_2$ regularizácie oproti $L_1$ regularizácie je, že
  sa ľahšie minimalizuje výsledný výraz. Ako príklad uvedieme lineárnu
  regresiu. V nej je hypotéza parametrizovaná stĺpcovým vektorom
  $\theta = (\theta_1, \ldots, \theta_n)^T$. Výstupom pre vstup
  $x = (x_1, \ldots, x_n)$ je $x \cdot \theta$. 
  
  Označme $X$ maticu, ktorej riadkami sú vstupe jednotlivých trénovacích
  príkladov. Ďalej nech $y$ je stĺpcový vektor cieľových výstupov na
  jednotlivých príkladoch. Ako určite vieme, optimálnymi parametrami
  lineárnej hypotézy je taký stĺpcový vektor $\theta$, ktorý je riešením
  rovnice
  $$X^T X \cdot \theta = X^T y.$$
  Dokáže, že keď k minimalizovanej hodnote pridáme pokutu vo forme
  $\lambda \cdot \norm{\theta}^2$, tak sa optimálnymi parametrami stane
  $\theta$ riešiace rovnicu
  $$(X^T X + \lambda I) \cdot \theta = X^T y.$$
  Rozmyslite si taktiež, že takýto explicitný ``vzorec'' nie je možné
  priamočiaro získať pre $L_1$ regularizáciu.
\end{exercise}
